{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b3bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to read files, get the stellar parameters, blur using a Gaussian kernel \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, glob\n",
    "from astropy.convolution import convolve, Gaussian1DKernel\n",
    "#gives file name without directories - for obtaining stellar parameters\n",
    "from pathlib import Path\n",
    "\n",
    "#fixed wavelength grid matching desi wavelength range \n",
    "#to fix the output dimension from the emulator - out_dim = len(WL_GRID) \n",
    "#fixed spacing too, seems appropriate for the desi wavelength range \n",
    "L_MIN, L_MAX, DLAM = 3600.0, 9824.0, 0.8\n",
    "\n",
    "#building the wavelength grid vector - used for emulator output axis \n",
    "def make_uniform_grid(lmin=L_MIN, lmax=L_MAX, dlam=DLAM):\n",
    "    #making it inclusive of lmax\n",
    "    #returns an array of wavelengths \n",
    "    n = int(np.floor((lmax - lmin)/dlam)) + 1\n",
    "    a = lmin + np.arange(n) * dlam\n",
    "    return a\n",
    "\n",
    "#def make_log_grid(lmin=3600., lmax=9800., R=3000., oversample=3):\n",
    "    # pixel size ~ FWHM/R/oversample in ln λ\n",
    "    #dln = 1.0/(R*oversample)\n",
    "    #n = int(np.floor(np.log(lmax/lmin)/dln)) + 1\n",
    "    #return lmin*np.exp(np.arange(n)*dln)\n",
    "\n",
    "#creating the grid\n",
    "WL_GRID = make_uniform_grid()\n",
    "\n",
    "#obtaining the stellar parameters from the file names - inputs to emulator\n",
    "#the pattern is based on the file names in the data directory - looks for substrings in the filenames \n",
    "# _T<Teff>_g<logg>_m<FeH>_a<aFe>_c<CFe>_n<NFe>\n",
    "# where <Teff>, <logg>, <FeH>, <aFe>, <CFe>, <NFe> are the stellar parameters \n",
    "# Teff in K, logg in cm/s^2... \n",
    "#Teff = positive float - the rest allow negative floats \n",
    "pattern = re.compile(\n",
    "    r\"_T(?P<Teff>\\d+(?:\\.\\d+)?)_g(?P<logg>-?\\d+(?:\\.\\d+)?)\"\n",
    "    r\"_m(?P<FeH>-?\\d+(?:\\.\\d+)?)_a(?P<aFe>-?\\d+(?:\\.\\d+)?)\"\n",
    "    r\"_c(?P<CFe>-?\\d+(?:\\.\\d+)?)_n(?P<NFe>-?\\d+(?:\\.\\d+)?)\")\n",
    "\n",
    "#function to read model file and obtain the stellar parameters \n",
    "#puts them in an array of shape (num_models, 6) \n",
    "#as well as the flux and wavlength arrays \n",
    "def read_model_dat(path):\n",
    "    #opening file and reading the data \n",
    "    #skipping two header lines, assinging column names \n",
    "    df = pd.read_csv(path, sep=r\"\\s+\", skiprows=2,\n",
    "                     names=[\"WAV\", \"FLUX\", \"CONT\", \"FLUX_CONT\"],\n",
    "                     dtype=float, engine=\"c\")\n",
    "    #converting wavelengths and fluxes to numpy arrays \n",
    "    wl = df[\"WAV\"].to_numpy()\n",
    "    flux = df[\"FLUX\"].to_numpy()\n",
    "    #to make sure that the wavelengths are increasing\n",
    "    #detects if any step is not increasing \n",
    "    if np.any(np.diff(wl) <= 0):\n",
    "        #sorting the wavelengths and fluxes \n",
    "        idx = np.argsort(wl)\n",
    "        wl, flux = wl[idx], flux[idx]\n",
    "    #applying the pattern to the file name \n",
    "    #tells if it doesnt match \n",
    "    m = pattern.search(Path(path).name)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Cannot parse params from {Path(path).name}\")\n",
    "    #turns captured groups into a dictionary with float values \n",
    "    d = {k: float(v) for k, v in m.groupdict().items()}\n",
    "    #put them into a 6 element vector in order of Teff, logg, FeH, aFe, CFe, NFe \n",
    "    params = np.array([d[\"Teff\"], d[\"logg\"], d[\"FeH\"], d[\"aFe\"], d[\"CFe\"], d[\"NFe\"]], float)\n",
    "    #returns the wavelength, flux and parameters \n",
    "    return wl, flux, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7de766",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spectral lines of desi broaded by instrument \n",
    "#apply gaussian blurring to the flux but need width in pixels \n",
    "def estimate_sigma_pixels(wl, R=3000.0):\n",
    "    \"\"\"converting resolving power R into gaussian sigma in pixels\"\"\" \n",
    "    #dlam = median wavelength step in Angstroms = angstroms per pixel \n",
    "    #use median to avoid outliers \n",
    "    dlam = np.median(np.diff(wl))\n",
    "    #converting resolving power to sigma in Angstroms \n",
    "    #R = lambda / fwhm - sigma = fwhm / 2.3548.. \n",
    "    sigma_A = (np.median(wl) / R) / 2.354820045\n",
    "    #converting sigma from angstroms to pixels by dividing by dlam = angs/pixel \n",
    "    #at least 0.5 pixels to avoid too narrow gaussian \n",
    "    return max(sigma_A / dlam, 0.5)\n",
    "\n",
    "#preprocessing for a single file \n",
    "#y output = what emulator will predict \n",
    "#info output = dictionary with parameters and intermediate data \n",
    "def preprocess_file_to_grid_logflux(path, wl_grid=WL_GRID, R=3000.0):\n",
    "    '''for each file: blur flux to DESI resolution,\n",
    "       trim to [wl_grid.min, wl_grid.max], rebin to wl_grid, log-transform\n",
    "       returns log(flux) on wl_grid and a dictionary with params + intermediate data''' \n",
    "    #reading the model data from the file \n",
    "    #returns wavelength, flux and parameters\n",
    "    #wl = wavelength in Angstroms, flux = flux in erg/s/cm^2\n",
    "    #params = [Teff, logg, FeH, aFe, CFe, NFe] \n",
    "    wl, flux, params = read_model_dat(path)\n",
    "    #finding gaussian width in pixels for spectrum - from resolving power \n",
    "    sig_px = estimate_sigma_pixels(wl, R=R)\n",
    "    #applying gaussian blur with the sigma in pixels\n",
    "    #building kernel in pixel units + extending the boundary so end flux not lost \n",
    "    #to match desi finite resolution \n",
    "    #synthetic models not - might get line width mismatches and biased fits \n",
    "    flux_blur = convolve(flux, Gaussian1DKernel(sig_px), boundary=\"extend\")\n",
    "    #masking the wavelengths that are outside the grid range - trims the spectrum \n",
    "    mask = (wl >= wl_grid[0]) & (wl <= wl_grid[-1])\n",
    "    wl_t, fx_t = wl[mask], flux_blur[mask]\n",
    "    #rebinning to the fixed wavelength grid\n",
    "    #linearly interpolating the blurred flux to the fixed wavelength grid\n",
    "    #anything that falls outside model coverage will be NaN\n",
    "    y_lin = np.interp(wl_grid, wl_t, fx_t, left=np.nan, right=np.nan)\n",
    "    #converting flux to log flux to avoid negative values\n",
    "    #clipping to avoid log(0) or log(negative) - set to 1e-6\n",
    "    y = np.log(np.clip(y_lin, 1e-6, None))\n",
    "    #returning the log flux on the fixed wavelength grid and a dictionary with parameters\n",
    "    #and the sigma in pixels for the Gaussian kernel\n",
    "    info = {\"params\": params, \"sigma_px\": sig_px}\n",
    "    return y, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4425b529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_targets(paths, wl_grid=WL_GRID, R=3000.0):\n",
    "    \"\"\"\n",
    "    Build training set from many files.\n",
    "    Returns:\n",
    "      Y : (N, Npix) matrix of log-flux spectra\n",
    "      P : (N, 6) parameter matrix\n",
    "    \"\"\"\n",
    "    Ys, Ps = [], []\n",
    "    for p in sorted(paths):\n",
    "        y, info = preprocess_file_to_grid_logflux(p, wl_grid, R)\n",
    "        Ys.append(y[None, :])\n",
    "        Ps.append(info[\"params\"][None, :])\n",
    "    Y, P = np.vstack(Ys), np.vstack(Ps)\n",
    "    return Y, P\n",
    "\n",
    "def compute_output_scaler(Y):\n",
    "    \"\"\"\n",
    "    Per-pixel mean/std for output standardisation.\n",
    "    \"\"\"\n",
    "    y_mean = np.nanmean(Y, axis=0)\n",
    "    y_std = np.nanstd(Y, axis=0) + 1e-6\n",
    "    return y_mean, y_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd366fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose only the base .dat files (ignore _broad.dat and .mod)\n",
    "files = [f for f in glob.glob(\"/export/linnunrata/jls/triple_models/*.dat\")\n",
    "         if not f.endswith(\"_broad.dat\") and not f.endswith(\".mod\")]\n",
    "\n",
    "# build training set\n",
    "Y, P = build_targets(files, WL_GRID, R=3000.0)\n",
    "\n",
    "# compute scalers\n",
    "y_mean, y_std = compute_output_scaler(Y)\n",
    "Y_std = (Y - y_mean) / y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5511ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emulator \n",
    "#want to take 6 parameters (Teff, logg, FeH, aFe, CFe, NFe) \n",
    "#predict a spectrum on fixed wavelengths grid \n",
    "\n",
    "#jax = like numpy but with autodiff - for computing gradients\n",
    "#jit = just-in-time compilation, speeds up the code - compiles the function to machine code\n",
    "#flax = neural networks library for JAX, similar to TensorFlow \n",
    "\n",
    "#importing the necessary libraries\n",
    "#jax version of numpy - can run on CPU, GPU, TPU \n",
    "import jax, jax.numpy as jnp\n",
    "#flax = deep learning library for JAX\n",
    "#linen = for defining neural networks - gives layers, activation functions, etc. \n",
    "from flax import linen as nn\n",
    "#python standard library for type hints\n",
    "#sequence [int] = a list of integers, represents the number of neurons in each layer\n",
    "from typing import Sequence\n",
    "\n",
    "\n",
    "#before running model \n",
    "#standardize inputs (subtract mean, divide by std for each of the 6–7 features)\n",
    "#keeps scales comparable and gradients well-behaved\n",
    "#after model: de-standardize the predicted spectrum to return to physical flux units on wavelength grid\n",
    "\n",
    "#converting data to and from standardized form \n",
    "#mean/std come from training data, computed for each feature \n",
    "#standardise = subtract mean and divide by standard deviation \n",
    "#keeping scales comparable - otherwise some features might dominate the gradients (e.g. Teff vs NFe)\n",
    "#destandardise = multiply by standard deviation and add mean - opposite\n",
    "#to get real flux units on the wavelength grid after prediction \n",
    "def standardise(x, mean, std, eps=1e-6): return (x - mean) / (std + eps)\n",
    "def destandardise(y, mean, std):         return y * std + mean\n",
    "\n",
    "\n",
    "class emulator(nn.Module):\n",
    "    #how many wavelengths to predict - the spectrum length \n",
    "    out_dim: int  \n",
    "    #widths of hidden layers - 5 layers with different sizes \n",
    "    hidden: Sequence[int] = (512,1024,2048,1024,512)\n",
    "    #random dropout rate - prevent overfitting by dropping neurons during training \n",
    "    dropout: float = 0.1\n",
    "\n",
    "    #forward pass of the model \n",
    "    #transformations applied to the input data with learnable weights \n",
    "    #x: input features - [Teff, logg, FeH, aFe, CFe, NFe] = (...,6) \n",
    "    #input : ... can be any number, but the last dimension must be 6 - vector of stellar parameters\n",
    "    #can have mutiple stars in a batch, so the input shape can be (2,6) \n",
    "\n",
    "    #compact = define the model layers in a concise way\n",
    "    #define sublayers in the __call__ method \n",
    "    @nn.compact\n",
    "    #__call__ = the forward pass of the model, takes input x and returns output\n",
    "    def __call__(self, x, *, train=False):    \n",
    "        h = x\n",
    "        #for each layer in the hidden layers\n",
    "        #apply a dense layer, layer normalization, ReLU activation, and dropout \n",
    "        for d in self.hidden:\n",
    "            #dense layers act only on batch, keeping the last dimension = 6 \n",
    "            #affine transformation, linear transformation with weights and biases \n",
    "            #h = h @ w + b - w = (inputs ,d), b = (d,) (d=2 for two stars) \n",
    "            #mixes all input features together linearly, to produce d new features \n",
    "            h = nn.Dense(d)(h)\n",
    "            #layer normalization = normalizes the output of the dense layer\n",
    "            #computing the mean and variance across features, not across the batch\n",
    "            #normalizes each feature to have zero mean and unit variance\n",
    "            #this helps with training stability and convergence\n",
    "            #h = (h - mean) / sqrt(var + eps)\n",
    "            #eps = small constant to avoid division by zero \n",
    "            h = nn.LayerNorm()(h)\n",
    "            #ReLU activation = rectified linear unit, applies non-linearity\n",
    "            #ReLU(x) = max(0, x) - sets negative values to zero \n",
    "            #network can do complex mappings - spectral lines shapes etc. \n",
    "            h = nn.relu(h)\n",
    "            #during training, apply dropout to the output of the ReLU activation\n",
    "            #dropout = randomly sets some neurons to zero, to prevent overfitting \n",
    "            #other neurons scale their output to keep the same expected value \n",
    "            #prevent network from relying too much on specific neurons \n",
    "            #only applied during training, not during inference \n",
    "            h = nn.Dropout(self.dropout, deterministic=not train)(h)\n",
    "        #after all hidden layers, apply a final dense layer to produce the output\n",
    "        #output shape = (batch_size, out_dim) = (2, 1000) for two stars and 1000 wavelengths e.g. 4000-8000 Angstroms\n",
    "        #this layer maps the final hidden representation to the output dimension\n",
    "        y = nn.Dense(self.out_dim)(h)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "30bd660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_flax_emulator.py\n",
    "import jax, jax.numpy as jnp\n",
    "import optax, pickle\n",
    "from flax.training.train_state import TrainState\n",
    "from flax.training import checkpoints\n",
    "\n",
    "def make_state(rng, out_dim, x_mean, x_std, y_mean, y_std, lr=1e-3):\n",
    "    model = FlaxEmulator(out_dim=out_dim)\n",
    "    params = model.init(rng, jnp.zeros((1,6)), train=True)\n",
    "    tx = optax.adamw(lr)\n",
    "    return TrainState.apply_fn==model.apply, params==params, tx==tx, opt_state==tx.init(params), \n",
    "    # Use a small helper container if your env expects a single object.\n",
    "\n",
    "def loss_fn(params, apply_fn, xb, yb, x_mean, x_std, y_mean, y_std):\n",
    "    xbn = standardize(xb, x_mean, x_std)\n",
    "    predn = apply_fn(params, xbn, train=True)\n",
    "    pred  = destandardize(predn, y_mean, y_std)\n",
    "    return jnp.mean((pred - yb)**2)\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, xb, yb, x_mean, x_std, y_mean, y_std):\n",
    "    l, grads = jax.value_and_grad(loss_fn)(state.params, state.apply_fn, xb, yb, x_mean, x_std, y_mean, y_std)\n",
    "    updates, new_opt_state = state.tx.update(grads, state.opt_state, state.params)\n",
    "    new_params = optax.apply_updates(state.params, updates)\n",
    "    return state.replace(params=new_params, opt_state=new_opt_state), l\n",
    "\n",
    "# Pseudocode usage:\n",
    "# X (N,6), Y (N, L), wav (L,), compute x_mean/std, y_mean/std, then loop train_step(...)\n",
    "# Save with checkpoints.save_checkpoint(..., target={'params':..., 'x_mean':..., 'x_std':..., 'y_mean':..., 'y_std':..., 'wav': wav})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c71db356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward_model.py\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def apply_extinction(flux, A_lambda, ebv):     # A_lambda per unit E(B-V) on the same wav grid\n",
    "    return flux * 10**(-0.4 * A_lambda * ebv)\n",
    "\n",
    "def blend(flux1, flux2, W):\n",
    "    return W * flux1 + (1. - W) * flux2\n",
    "\n",
    "def emulator_predict(params_emul, apply_fn, x_mean, x_std, y_mean, y_std, theta6):\n",
    "    x = (theta6 - x_mean) / (x_std + 1e-6)\n",
    "    yn = apply_fn(params_emul, x, train=False)\n",
    "    return yn * y_std + y_mean\n",
    "\n",
    "def model_fibre(params_emul, apply_fn, stats, theta1_6, ebv1, theta2_6, ebv2, W, A_lambda):\n",
    "    spec1 = emulator_predict(params_emul, apply_fn, stats['x_mean'], stats['x_std'], stats['y_mean'], stats['y_std'], theta1_6)\n",
    "    spec2 = emulator_predict(params_emul, apply_fn, stats, theta2_6)  # or pass stats fields explicitly\n",
    "    spec1 = apply_extinction(spec1, A_lambda, ebv1)\n",
    "    spec2 = apply_extinction(spec2, A_lambda, ebv2)\n",
    "    return blend(spec1, spec2, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dc86673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# photometry_jax.py\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def trapz(y, x): dx = jnp.diff(x); return jnp.sum(0.5*(y[...,1:]+y[...,:-1])*dx, axis=-1)\n",
    "\n",
    "def synth_mags(wav_um, flux, filters):\n",
    "    \"\"\"filters: dict[name] -> (T_on_wav, zp_offset). T already on wav_um grid; AB by default.\"\"\"\n",
    "    c = 2.99792458e10\n",
    "    mags = {}\n",
    "    for name, (T, zp) in filters.items():\n",
    "        w = wav_um\n",
    "        num = trapz(flux * T * w, w)      # photon counting\n",
    "        den = trapz(T * w, w) + 1e-30\n",
    "        f_lambda = num / den\n",
    "        lam_cm = jnp.mean(w) * 1e-4\n",
    "        f_nu = f_lambda * lam_cm**2 / c\n",
    "        mags[name] = -2.5*jnp.log10(jnp.clip(f_nu, 1e-50)) - 48.6 + zp\n",
    "    return mags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "989a73f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logposterior.py\n",
    "import jax, jax.numpy as jnp\n",
    "from jax.nn import logsumexp\n",
    "\n",
    "def loglike_spec(obs_flux, obs_ivar, mod_flux):\n",
    "    resid = obs_flux - mod_flux\n",
    "    return -0.5 * jnp.sum(resid**2 * obs_ivar)\n",
    "\n",
    "def loglike_phot(m_obs, m_pred, sigma):\n",
    "    return -0.5 * jnp.sum((m_obs - m_pred)**2 / (sigma**2))\n",
    "\n",
    "def build_logposterior(apply_fn, params_emul, stats, wav_um, A_lambda, filters, priors):\n",
    "    def unpack(theta):\n",
    "        # theta = [θ1(6), ebv1, θ2(6), ebv2, W_A, W_B]\n",
    "        t1 = theta[0:6]; ebv1 = theta[6]\n",
    "        t2 = theta[7:13]; ebv2 = theta[13]\n",
    "        W_A = jnp.clip(theta[14], 0., 1.); W_B = jnp.clip(theta[15], 0., 1.)\n",
    "        return t1, ebv1, t2, ebv2, W_A, W_B\n",
    "\n",
    "    def fibre_loglike(t1, ebv1, t2, ebv2, W_A, W_B, data):\n",
    "        from forward_model import model_fibre\n",
    "        mod_A = model_fibre(params_emul, apply_fn, stats, t1, ebv1, t2, ebv2, W_A, A_lambda)\n",
    "        mod_B = model_fibre(params_emul, apply_fn, stats, t1, ebv1, t2, ebv2, W_B, A_lambda)\n",
    "        ll = loglike_spec(data['A']['flux'], data['A']['ivar'], mod_A)\n",
    "        ll+= loglike_spec(data['B']['flux'], data['B']['ivar'], mod_B)\n",
    "\n",
    "        if 'phot' in data:\n",
    "            from photometry_jax import synth_mags\n",
    "            blend_for_phot = 0.5*(mod_A + mod_B)   # or a chosen mapping from fibres → imaging\n",
    "            mpred = synth_mags(wav_um, blend_for_phot, filters)\n",
    "            m_pred_vec = jnp.array([mpred[b] for b in data['phot']['bands']])\n",
    "            ll += loglike_phot(data['phot']['m'], m_pred_vec, data['phot']['sigma'])\n",
    "        return ll\n",
    "\n",
    "    def logprior(t1, ebv1, t2, ebv2, W_A, W_B):\n",
    "        return (priors['star'](t1) + priors['ebv'](ebv1) +\n",
    "                priors['star'](t2) + priors['ebv'](ebv2) +\n",
    "                priors['W'](W_A)   + priors['W'](W_B))\n",
    "\n",
    "    def logpost(theta, data):\n",
    "        t1, e1, t2, e2, WA, WB = unpack(theta)\n",
    "        # two labelings: (1,2) and (2,1)\n",
    "        lp  = logprior(t1,e1,t2,e2,WA,WB)\n",
    "        ll1 = fibre_loglike(t1,e1,t2,e2,WA,WB, data)\n",
    "        ll2 = fibre_loglike(t2,e2,t1,e1,WA,WB, data)\n",
    "        # permutation-safe combine\n",
    "        return logsumexp(jnp.array([lp + ll1, lp + ll2]))\n",
    "    return logpost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4654a5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuts_driver.py\n",
    "import jax, jax.numpy as jnp, blackjax\n",
    "\n",
    "def run_nuts(logpost, data, theta_init, n_warm=1000, n_samp=2000, seed=0):\n",
    "    def logprob(theta): return logpost(theta, data)\n",
    "    rng = jax.random.PRNGKey(seed)\n",
    "    kernel = blackjax.nuts(logprob)\n",
    "    state = kernel.init(theta_init, rng)\n",
    "\n",
    "    @jax.jit\n",
    "    def one_step(state, key):\n",
    "        new_state, info = kernel.step(key, state)\n",
    "        return new_state, info\n",
    "\n",
    "    samples = []\n",
    "    key = rng\n",
    "    for i in range(n_warm + n_samp):\n",
    "        key, sub = jax.random.split(key)\n",
    "        state, _ = one_step(state, sub)\n",
    "        if i >= n_warm:\n",
    "            samples.append(state.position)\n",
    "    return jnp.stack(samples, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "428caae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# priors.py\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def box_gauss_vec(x, mu, sig, lo, hi):\n",
    "    inside = jnp.all((x>=lo)&(x<=hi))\n",
    "    return jnp.where(inside, -0.5*jnp.sum(((x-mu)/sig)**2), -jnp.inf)\n",
    "\n",
    "def star_prior_factory(ranges):\n",
    "    lo = jnp.array([a for a,b in ranges]); hi = jnp.array([b for a,b in ranges])\n",
    "    mu = 0.5*(lo+hi); sig = (hi-lo)/3.\n",
    "    return lambda x: box_gauss_vec(x, mu, sig, lo, hi)\n",
    "\n",
    "def ebv_prior(mu=0.07, sig=0.05, lo=0., hi=1.0):\n",
    "    def lp(x):\n",
    "        inside = (x>=lo)&(x<=hi)\n",
    "        return jnp.where(inside, -0.5*((x-mu)/sig)**2, -jnp.inf)\n",
    "    return lp\n",
    "\n",
    "def beta01(a=1., b=1.):\n",
    "    from jax.scipy.special import betaln\n",
    "    Z = betaln(a,b)\n",
    "    def lp(w):\n",
    "        inside = (0.<=w)*(w<=1.)\n",
    "        return jnp.where(inside, (a-1.)*jnp.log(jnp.clip(w,1e-12,1.)) + (b-1.)*jnp.log(jnp.clip(1.-w,1e-12,1.)) - Z, -jnp.inf)\n",
    "    return lp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aceb1668",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 1) Load emulator checkpoint and stats\u001b[39;00m\n\u001b[32m      6\u001b[39m ckpt = checkpoints.restore_checkpoint(\u001b[33m\"\u001b[39m\u001b[33memulator_ckpt_dir\u001b[39m\u001b[33m\"\u001b[39m, target=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m params_emul = \u001b[43mckpt\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mparams\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      8\u001b[39m x_mean, x_std = ckpt[\u001b[33m'\u001b[39m\u001b[33mx_mean\u001b[39m\u001b[33m'\u001b[39m], ckpt[\u001b[33m'\u001b[39m\u001b[33mx_std\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      9\u001b[39m y_mean, y_std = ckpt[\u001b[33m'\u001b[39m\u001b[33my_mean\u001b[39m\u001b[33m'\u001b[39m], ckpt[\u001b[33m'\u001b[39m\u001b[33my_std\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# run_inference.py\n",
    "import jax.numpy as jnp, jax\n",
    "from flax.training import checkpoints\n",
    "\n",
    "# 1) Load emulator checkpoint and stats\n",
    "ckpt = checkpoints.restore_checkpoint(\"emulator_ckpt_dir\", target=None)\n",
    "params_emul = ckpt['params']\n",
    "x_mean, x_std = ckpt['x_mean'], ckpt['x_std']\n",
    "y_mean, y_std = ckpt['y_mean'], ckpt['y_std']\n",
    "wav_um       = ckpt['wav']          # ensure this matches DESI grid used for data\n",
    "\n",
    "emulator = FlaxEmulator(out_dim=len(wav_um))\n",
    "apply_fn = emulator.apply\n",
    "stats = {'x_mean': x_mean, 'x_std': x_std, 'y_mean': y_mean, 'y_std': y_std}\n",
    "\n",
    "# 2) Prepare data dict\n",
    "data = {\n",
    "  'A': {'flux': flux_A, 'ivar': ivar_A},\n",
    "  'B': {'flux': flux_B, 'ivar': ivar_B},\n",
    "  'phot': {'m': m_obs, 'sigma': m_err, 'bands': bands},   # optional\n",
    "}\n",
    "A_lambda = A_lambda_on_wav      # array length = len(wav_um)\n",
    "filters  = preinterp_filters    # dict: name -> (T_on_wav, zp)\n",
    "\n",
    "# 3) Priors\n",
    "ranges = jnp.array([\n",
    "    [4000., 6500.],   # Teff\n",
    "    [3.5, 5.0],       # logg\n",
    "    [-2.5, 0.5],      # [Fe/H]\n",
    "    [-0.2, 0.5],      # [alpha/Fe]\n",
    "    [-0.5, 2.0],      # [C/Fe]\n",
    "    [-0.5, 2.0],      # [N/Fe]\n",
    "])\n",
    "priors = {'star': star_prior_factory(ranges),\n",
    "          'ebv':  ebv_prior(0.07,0.05,0.,1.0),\n",
    "          'W':    beta01(1.,1.)}\n",
    "\n",
    "# 4) Build log-posterior closure\n",
    "logpost = build_logposterior(apply_fn, params_emul, stats, wav_um, A_lambda, filters, priors)\n",
    "\n",
    "# 5) Initialisation (use photometry-only predictions if you have them)\n",
    "theta0 = jnp.array([teff1,logg1,feh1,a1,c1,n1, ebv1,\n",
    "                    teff2,logg2,feh2,a2,c2,n2, ebv2,\n",
    "                    0.7, 0.3])\n",
    "\n",
    "# 6) Sample\n",
    "samples = run_nuts(logpost, data, theta0, n_warm=1000, n_samp=2000, seed=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841c26a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
