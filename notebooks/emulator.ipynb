{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16b3bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to read files, get the stellar parameters, blur using a Gaussian kernel \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, glob\n",
    "from astropy.convolution import convolve, Gaussian1DKernel\n",
    "#gives file name without directories - for obtaining stellar parameters\n",
    "from pathlib import Path\n",
    "\n",
    "#fixed wavelength grid matching desi wavelength range \n",
    "#to fix the output dimension from the emulator - out_dim = len(WL_GRID) \n",
    "#fixed spacing too, seems appropriate for the desi wavelength range \n",
    "L_MIN, L_MAX, DLAM = 3600.0, 9824.0, 0.8\n",
    "\n",
    "#building the wavelength grid vector - used for emulator output axis \n",
    "def make_uniform_grid(lmin=L_MIN, lmax=L_MAX, dlam=DLAM):\n",
    "    #making it inclusive of lmax\n",
    "    #returns an array of wavelengths \n",
    "    n = int(np.floor((lmax - lmin)/dlam)) + 1\n",
    "    a = lmin + np.arange(n) * dlam\n",
    "    return a\n",
    "\n",
    "#def make_log_grid(lmin=3600., lmax=9800., R=3000., oversample=3):\n",
    "    # pixel size ~ FWHM/R/oversample in ln Î»\n",
    "    #dln = 1.0/(R*oversample)\n",
    "    #n = int(np.floor(np.log(lmax/lmin)/dln)) + 1\n",
    "    #return lmin*np.exp(np.arange(n)*dln)\n",
    "\n",
    "#creating the grid\n",
    "WL_GRID = make_uniform_grid()\n",
    "\n",
    "#obtaining the stellar parameters from the file names - inputs to emulator\n",
    "#the pattern is based on the file names in the data directory - looks for substrings in the filenames \n",
    "# _T<Teff>_g<logg>_m<FeH>_a<aFe>_c<CFe>_n<NFe>\n",
    "# where <Teff>, <logg>, <FeH>, <aFe>, <CFe>, <NFe> are the stellar parameters \n",
    "# Teff in K, logg in cm/s^2... \n",
    "#Teff = positive float - the rest allow negative floats \n",
    "pattern = re.compile(\n",
    "    r\"_T(?P<Teff>\\d+(?:\\.\\d+)?)_g(?P<logg>-?\\d+(?:\\.\\d+)?)\"\n",
    "    r\"_m(?P<FeH>-?\\d+(?:\\.\\d+)?)_a(?P<aFe>-?\\d+(?:\\.\\d+)?)\"\n",
    "    r\"_c(?P<CFe>-?\\d+(?:\\.\\d+)?)_n(?P<NFe>-?\\d+(?:\\.\\d+)?)\")\n",
    "\n",
    "#function to read model file and obtain the stellar parameters \n",
    "#puts them in an array of shape (num_models, 6) \n",
    "#as well as the flux and wavlength arrays \n",
    "def read_model_dat(path):\n",
    "    #opening file and reading the data \n",
    "    #skipping two header lines, assinging column names \n",
    "    df = pd.read_csv(path, sep=r\"\\s+\", skiprows=2,\n",
    "                     names=[\"WAV\", \"FLUX\", \"CONT\", \"FLUX_CONT\"],\n",
    "                     dtype=float, engine=\"c\")\n",
    "    #converting wavelengths and fluxes to numpy arrays \n",
    "    wl = df[\"WAV\"].to_numpy()\n",
    "    flux = df[\"FLUX\"].to_numpy()\n",
    "    #to make sure that the wavelengths are increasing\n",
    "    #detects if any step is not increasing \n",
    "    if np.any(np.diff(wl) <= 0):\n",
    "        #sorting the wavelengths and fluxes \n",
    "        idx = np.argsort(wl)\n",
    "        wl, flux = wl[idx], flux[idx]\n",
    "    #applying the pattern to the file name \n",
    "    #tells if it doesnt match \n",
    "    m = pattern.search(Path(path).name)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Cannot parse params from {Path(path).name}\")\n",
    "    #turns captured groups into a dictionary with float values \n",
    "    d = {k: float(v) for k, v in m.groupdict().items()}\n",
    "    #put them into a 6 element vector in order of Teff, logg, FeH, aFe, CFe, NFe \n",
    "    params = np.array([d[\"Teff\"], d[\"logg\"], d[\"FeH\"], d[\"aFe\"], d[\"CFe\"], d[\"NFe\"]], float)\n",
    "    #returns the wavelength, flux and parameters \n",
    "    return wl, flux, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c7de766",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spectral lines of desi broaded by instrument \n",
    "#apply gaussian blurring to the flux but need width in pixels \n",
    "def estimate_sigma_pixels(wl, R=3000.0):\n",
    "    \"\"\"converting resolving power R into gaussian sigma in pixels\"\"\" \n",
    "    #dlam = median wavelength step in Angstroms = angstroms per pixel \n",
    "    #use median to avoid outliers \n",
    "    dlam = np.median(np.diff(wl))\n",
    "    #converting resolving power to sigma in Angstroms \n",
    "    #R = lambda / fwhm - sigma = fwhm / 2.3548.. \n",
    "    FWHM_over_sigma = 2*np.sqrt(2*np.log(2))\n",
    "    sigma_A = (np.median(wl) / R) / FWHM_over_sigma\n",
    "    #converting sigma from angstroms to pixels by dividing by dlam = angs/pixel \n",
    "    #at least 0.5 pixels to avoid too narrow gaussian \n",
    "    return max(sigma_A / dlam, 0.3)\n",
    "\n",
    "#preprocessing for a single file \n",
    "#y output = what emulator will predict \n",
    "#info output = dictionary with parameters and intermediate data \n",
    "def preprocess_file_to_grid_logflux(path, wl_grid=WL_GRID, R=3000.0):\n",
    "    '''for each file: blur flux to DESI resolution,\n",
    "       trim to [wl_grid.min, wl_grid.max], rebin to wl_grid, log-transform\n",
    "       returns log(flux) on wl_grid and a dictionary with params + intermediate data''' \n",
    "    #reading the model data from the file \n",
    "    #returns wavelength, flux and parameters\n",
    "    #wl = wavelength in Angstroms, flux = flux in erg/s/cm^2\n",
    "    #params = [Teff, logg, FeH, aFe, CFe, NFe] \n",
    "    wl, flux, params = read_model_dat(path)\n",
    "    #finding gaussian width in pixels for spectrum - from resolving power \n",
    "    sig_px = estimate_sigma_pixels(wl, R=R)\n",
    "    #applying gaussian blur with the sigma in pixels\n",
    "    #building kernel in pixel units + extending the boundary so end flux not lost \n",
    "    #to match desi finite resolution \n",
    "    #synthetic models not - might get line width mismatches and biased fits \n",
    "    flux_blur = convolve(flux, Gaussian1DKernel(sig_px), boundary=\"extend\")\n",
    "    #masking the wavelengths that are outside the grid range - trims the spectrum \n",
    "    mask = (wl >= wl_grid[0]) & (wl <= wl_grid[-1])\n",
    "    wl_t, fx_t = wl[mask], flux_blur[mask]\n",
    "    #rebinning to the fixed wavelength grid\n",
    "    #linearly interpolating the blurred flux to the fixed wavelength grid\n",
    "    #anything that falls outside model coverage will be NaN\n",
    "    y_lin = np.interp(wl_grid, wl_t, fx_t, left=np.nan, right=np.nan)\n",
    "    #converting flux to log flux to avoid negative values\n",
    "    #clipping to avoid log(0) or log(negative) - set to 1e-6\n",
    "    y = np.log(np.clip(y_lin, 1e-6, None))\n",
    "    #returning the log flux on the fixed wavelength grid and a dictionary with parameters\n",
    "    #and the sigma in pixels for the Gaussian kernel\n",
    "    info = {\"params\": params, \"sigma_px\": sig_px}\n",
    "    return y, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4425b529",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looping over many model files to build the training set \n",
    "#Y = targets matrix of shape (N, Npix) - log-flux spectra\n",
    "#P = parameters matrix of shape (N, 6) - stellar parameters \n",
    "def build_targets(paths, wl_grid=WL_GRID, R=3000.0):\n",
    "    \"\"\"returns (N, Npix) matrix of log-flux spectra and (N, 6) parameter matrix\"\"\"\n",
    "    #two lists to store the outputs \n",
    "    Ys, Ps = [], []\n",
    "    #looping over the sorted paths to ensure consistent order \n",
    "    for p in sorted(paths):\n",
    "        #reading file, trims, blurs, rebins, applying log\n",
    "        y, info = preprocess_file_to_grid_logflux(p, wl_grid, R)\n",
    "        #turning 1D array into a row shape - (1, Npix)\n",
    "        #and appending to the list of Ys\n",
    "        Ys.append(y[None, :])\n",
    "        #appending the parameters to the list of Ps - from info dict\n",
    "        Ps.append(info[\"params\"][None, :])\n",
    "    #stacking the lists into 2D arrays \n",
    "    #(n_files, Npix) and (n_files, 6) shapes\n",
    "    Y, P = np.vstack(Ys), np.vstack(Ps)\n",
    "    return Y, P\n",
    "\n",
    "#computing one mean and std per wavelength pixel\n",
    "#stabalise training \n",
    "def compute_output_scaler(Y):\n",
    "    \"\"\"per-pixel mean/std for output standardisation.\"\"\"\n",
    "    #column wise mean over npix,.. \n",
    "    y_mean = np.nanmean(Y, axis=0)\n",
    "    #column wise std over (npix,..) + small constant to avoid division by zero \n",
    "    y_std = np.nanstd(Y, axis=0) + 1e-6\n",
    "    return y_mean, y_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd366fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose only the base .dat files (ignore _broad.dat and .mod)\n",
    "files = [f for f in glob.glob(\"/export/linnunrata/jls/triple_models/*.dat\")\n",
    "         if not f.endswith(\"_broad.dat\") and not f.endswith(\".mod\")]\n",
    "\n",
    "#build training set\n",
    "Y, P = build_targets(files, WL_GRID, R=3000.0)\n",
    "\n",
    "#compute scalers \n",
    "#y_mean = av log flux at each wavelength\n",
    "#y_std = std log flux at each wavelength + small constant to avoid division by zero \n",
    "#stablising gradients to avoid big variance in the training set\n",
    "y_mean, y_std = compute_output_scaler(Y)\n",
    "#Y_std = what emulator learns to predict\n",
    "#standardising the output by subtracting mean and dividing by std\n",
    "#Y = (Y - y_mean) / y_std\n",
    "Y_std = (Y - y_mean) / y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5511ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emulator \n",
    "#want to take 6 parameters (Teff, logg, FeH, aFe, CFe, NFe) \n",
    "#predict a spectrum on fixed wavelengths grid \n",
    "\n",
    "#jax = like numpy but with autodiff - for computing gradients\n",
    "#jit = just-in-time compilation, speeds up the code - compiles the function to machine code\n",
    "#flax = neural networks library for JAX, similar to TensorFlow \n",
    "\n",
    "#importing the necessary libraries\n",
    "#jax version of numpy - can run on CPU, GPU, TPU \n",
    "import jax, jax.numpy as jnp\n",
    "#flax = deep learning library for JAX\n",
    "#linen = for defining neural networks - gives layers, activation functions, etc. \n",
    "from flax import linen as nn\n",
    "#python standard library for type hints\n",
    "#sequence [int] = a list of integers, represents the number of neurons in each layer\n",
    "from typing import Sequence\n",
    "\n",
    "\n",
    "#before running model \n",
    "#standardize inputs (subtract mean, divide by std for each of the 6â7 features)\n",
    "#keeps scales comparable and gradients well-behaved\n",
    "#after model: de-standardize the predicted spectrum to return to physical flux units on wavelength grid\n",
    "\n",
    "#converting data to and from standardized form \n",
    "#mean/std come from training data, computed for each feature \n",
    "#standardise = subtract mean and divide by standard deviation \n",
    "#keeping scales comparable - otherwise some features might dominate the gradients (e.g. Teff vs NFe)\n",
    "#destandardise = multiply by standard deviation and add mean - opposite\n",
    "#to get real flux units on the wavelength grid after prediction \n",
    "def standardise(x, mean, std, eps=1e-6): return (x - mean) / (std + eps)\n",
    "def destandardise(y, mean, std):         return y * std + mean\n",
    "\n",
    "\n",
    "class emulator(nn.Module):\n",
    "    #how many wavelengths to predict - the spectrum length \n",
    "    out_dim: int  \n",
    "    #widths of hidden layers - 5 layers with different sizes \n",
    "    hidden: Sequence[int] = (512,1024,2048,1024,512)\n",
    "    #random dropout rate - prevent overfitting by dropping neurons during training \n",
    "    dropout: float = 0.1\n",
    "\n",
    "    #forward pass of the model \n",
    "    #transformations applied to the input data with learnable weights \n",
    "    #x: input features - [Teff, logg, FeH, aFe, CFe, NFe] = (...,6) \n",
    "    #input : ... can be any number, but the last dimension must be 6 - vector of stellar parameters\n",
    "    #can have mutiple stars in a batch, so the input shape can be (2,6) \n",
    "\n",
    "    #compact = define the model layers in a concise way\n",
    "    #define sublayers in the __call__ method \n",
    "    @nn.compact\n",
    "    #__call__ = the forward pass of the model, takes input x and returns output\n",
    "    def __call__(self, x, *, train=False):    \n",
    "        h = x\n",
    "        #for each layer in the hidden layers\n",
    "        #apply a dense layer, layer normalization, ReLU activation, and dropout \n",
    "        for d in self.hidden:\n",
    "            #dense layers act only on batch, keeping the last dimension = 6 \n",
    "            #affine transformation, linear transformation with weights and biases \n",
    "            #h = h @ w + b - w = (inputs ,d), b = (d,) (d=2 for two stars) \n",
    "            #mixes all input features together linearly, to produce d new features \n",
    "            h = nn.Dense(d)(h)\n",
    "            #layer normalization = normalizes the output of the dense layer\n",
    "            #computing the mean and variance across features, not across the batch\n",
    "            #normalizes each feature to have zero mean and unit variance\n",
    "            #this helps with training stability and convergence\n",
    "            #h = (h - mean) / sqrt(var + eps)\n",
    "            #eps = small constant to avoid division by zero \n",
    "            h = nn.LayerNorm()(h)\n",
    "            #ReLU activation = rectified linear unit, applies non-linearity\n",
    "            #ReLU(x) = max(0, x) - sets negative values to zero \n",
    "            #network can do complex mappings - spectral lines shapes etc. \n",
    "            h = nn.relu(h)\n",
    "            #during training, apply dropout to the output of the ReLU activation\n",
    "            #dropout = randomly sets some neurons to zero, to prevent overfitting \n",
    "            #other neurons scale their output to keep the same expected value \n",
    "            #prevent network from relying too much on specific neurons \n",
    "            #only applied during training, not during inference \n",
    "            h = nn.Dropout(self.dropout, deterministic=not train)(h)\n",
    "        #after all hidden layers, apply a final dense layer to produce the output\n",
    "        #output shape = (batch_size, out_dim) = (2, 1000) for two stars and 1000 wavelengths e.g. 4000-8000 Angstroms\n",
    "        #this layer maps the final hidden representation to the output dimension\n",
    "        y = nn.Dense(self.out_dim)(h)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f783d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
